{\rtf1\ansi\ansicpg1252\cocoartf1561\cocoasubrtf200
{\fonttbl\f0\fswiss\fcharset0 Helvetica;\f1\fnil\fcharset0 Monaco;}
{\colortbl;\red255\green255\blue255;\red0\green0\blue0;\red255\green255\blue255;\red109\green109\blue109;
}
{\*\expandedcolortbl;;\cssrgb\c0\c0\c0;\cssrgb\c100000\c100000\c100000;\cssrgb\c50196\c50196\c50196;
}
\margl1440\margr1440\vieww16180\viewh11840\viewkind0
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f0\fs24 \cf0 Clayton Corbin\
CSCE 587\
Take Home Midterm\
\

\b ~ Part 1 ~\

\b0 \
1) Cast column 9 to factor\
\
> abaloneData <- read.csv('abalone.csv', header=FALSE)\
> abaloneData$V9 = as.factor(abaloneData$V9)\
\
2) Partition data set\
\
> abaloneTrainData <- abaloneData[1:1392,]\
> abaloneTestData <- abaloneData[1393:4177,]\
\

\b ~ Part 2 ~\

\b0 \
1) Train model\
\
We want to train the model using the first 8 columns as the independent features and column 9 as the dependent feature\
> m <- naiveBayes(abaloneTrainData[,1:8], abaloneTrainData[,9])\
\
\
2) Create confusion matrix\
\
> confMat <- table(predict(m, abaloneTestData[,1:8]), abaloneTestData[,9])\
\

\itap1\trowd \taflags0 \trgaph108\trleft-108 \trcbpat3 \trbrdrt\brdrnil \trbrdrl\brdrnil \trbrdrt\brdrnil \trbrdrr\brdrnil \trpadl120 \tapadb160 
\clvertalt \clshdrawnil \clwWidth16018\clftsWidth3 \clheight364 \clbrdrt\brdrnil \clbrdrl\brdrnil \clbrdrb\brdrnil \clbrdrr\brdrnil \clpadl0 \clpadr0 \gaph\cellx8640
\pard\intbl\itap1\pardeftab720\sl340\partightenfactor0

\f1 \cf2 \expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 \cell \lastrow\row
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f0 \cf0 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 3) Using predict() on our test data and summing the diagonal of the confusion matrix and diving by the sum of all other entries. Accuracy = .1846264 \
(~ 18.46%)\
\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 > accuracy <- (sum(diag(confMat))) / sum(confMat)\
> accuracy\
\pard\pardeftab720\sl340\partightenfactor0

\f1 \cf2 \expnd0\expndtw0\kerning0
[1] 0.1846264\
\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f0\b \cf0 \kerning1\expnd0\expndtw0 ~ Part 3 ~\

\b0 \
1) Using rpart() method to train DT using training set\
\
> tree1 = rpart(V9 ~ V1 + V2 + V3 + V4 + V5 + V6 + V7 + V8, method = "class", data=abaloneTrainData, control=rpart.control(minsplit=2, cp=0.001))\
\
2) Create confusion matrix\
\
> treeConfMatrix <- table(predict(tree1,abaloneTestData[,1:8], type="class"),abaloneTestData[,9])\
\
3) Calculate accuracy. Accuracy = .2133621 (~ 21.34)\
\
> accuracyTree <- (sum(diag(treeConfMatrix))) / sum(treeConfMatrix)\
> accuracyTree\

\itap1\trowd \taflags0 \trgaph108\trleft-108 \trcbpat3 \trbrdrt\brdrnil \trbrdrl\brdrnil \trbrdrr\brdrnil \trpadl120 \tapadb160 
\clvertalt \clshdrawnil \clwWidth16018\clftsWidth3 \clbrdrt\brdrnil \clbrdrl\brdrnil \clbrdrb\brdrnil \clbrdrr\brdrnil \clpadl0 \clpadr0 \gaph\cellx8640
\pard\intbl\itap1\pardeftab720\sl340\partightenfactor0

\f1 \cf2 \expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 [1] 0.2133621\cell \row

\itap1\trowd \taflags0 \trgaph108\trleft-108 \trcbpat3 \trbrdrl\brdrnil \trbrdrr\brdrnil \trpadl120 \tapadb160 
\clvertalt \clshdrawnil \clwWidth16018\clftsWidth3 \clbrdrt\brdrnil \clbrdrl\brdrnil \clbrdrb\brdrnil \clbrdrr\brdrnil \clpadl0 \clpadr0 \gaph\cellx8640
\pard\intbl\itap1\pardeftab720\sl340\partightenfactor0
\cf2 \cell \row

\itap1\trowd \taflags0 \trgaph108\trleft-108 \trcbpat3 \trbrdrl\brdrnil \trbrdrt\brdrnil \trbrdrr\brdrnil \trpadl120 \tapadb160 
\clvertalt \clshdrawnil \clwWidth16018\clftsWidth3 \clbrdrt\brdrnil \clbrdrl\brdrnil \clbrdrb\brdrnil \clbrdrr\brdrnil \clpadl0 \clpadr0 \gaph\cellx8640

\itap2\trowd \taflags0 \trgaph108\trleft-108 \trbrdrt\brdrnil \trbrdrl\brdrnil \trbrdrt\brdrnil \trbrdrr\brdrnil 
\clvertalt \clshdrawnil \clwWidth16018\clftsWidth3 \clbrdrt\brdrnil \clbrdrl\brdrnil \clbrdrb\brdrnil \clbrdrr\brdrnil \clpadl0 \clpadr0 \gaph\cellx8640
\pard\intbl\itap2\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f0\b \cf0 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 ~ Part 4 ~\

\b0 \
Most of the numerical values in the abalone data set are < 1. When using naive bayes, we are multiplying a lot of probabilities. This can result in floating-point underflow which reduces the accuracy of naive bayes.\
\pard\intbl\itap2\pardeftab720\sl340\partightenfactor0

\f1 \cf2 \expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 \nestcell \lastrow\nestrow\cell \lastrow\row
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf2 \outl0\strokewidth0 \
\pard\pardeftab720\sl340\partightenfactor0
\cf2 \
}