{\rtf1\ansi\ansicpg1252\cocoartf1561\cocoasubrtf200
{\fonttbl\f0\fswiss\fcharset0 Helvetica;\f1\fnil\fcharset0 Monaco;}
{\colortbl;\red255\green255\blue255;\red0\green0\blue255;\red0\green0\blue0;\red255\green255\blue255;
\red109\green109\blue109;}
{\*\expandedcolortbl;;\cssrgb\c0\c0\c100000;\cssrgb\c0\c0\c0;\cssrgb\c100000\c100000\c100000;
\cssrgb\c50196\c50196\c50196;}
\margl1440\margr1440\vieww15120\viewh13980\viewkind0
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f0\fs24 \cf0 Clayton Corbin\
CSCE 587\
March 6th, 2018\
Linear/Logistic Regression\
\
~ Part 1 ~\
\

\b a)
\b0 \
\
First, renamed the first column to \'93Y\'94 and the second to \'93X\'94 for clarity purposes.\
\
> colnames(gold_target1)[1] <- "Y"\
> colnames(gold_target1)[2] <- \'93X\'94\
\
Then assigned these columns to variables (X and Y)\
\
> X <- gold_target1[['X']]\
> Y <- gold_target1[['Y']]\
\
plot\
\
> plot(Y~X)\
\

\b b)
\b0 \
\
Fit with linear model\
\
> m = lm(Y~X)\
\

\b c)\
\

\b0 > par(mfrow=c(2,2))\
> plot(m)\
\

\b d) 
\b0 What does this tell us about the fit of our model?\
\
Residuals vs Fitted - It seems as though our model indicates here that the data is mostly linear given that the residuals are evenly distributed about the 0 line and there may be a constant variance. However, the further you go right in the graph, the residuals start to separate from the 0 line which could mean potential outliers.\
\
Normal Q-Q - This plot shows a distribution with heavy tails meaning there are some more extreme values than anticipated with a normal distribution.\
\
Scale-Location - This plot indicates that the residuals are not spread so equally along the ranges of predictors. There may not be equal variance.\
\
Residuals vs Leverage - In this plot, Cook\'92s lines are fairly visible and \
there is an influential observation at #63 indicating a (possibly) influential outlier.\
\

\b e)\
\

\b0 Visualize predicted and observed y values.\
\
> ypred = predict(m)\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 > par(mfrow=c(1,1))\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 > plot(Y,Y,type='l', xlab="observed y", ylab="predicted y")\
> points(Y, ypred)\
\
~ Part 2 ~\
\

\b a)\
\

\b0 I didn\'92t rename the columns for this part and left the initial ones which got a bit confusing but I\'92m confident that I still plotted these steps properly. Here, \'930_1\'94 is the 4th column described in step \'93a\'94 as (Y). Column 1 is labeled as \'93Y\'94 but is actually the 1st column and described as (X).\
\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 > plot(gold_target1[['0_1']]~gold_target1[[\'91Y\'92]])\
\

\b b)\
\

\b0 Same thing here, column 4 against column 2 (which is labeled as X).\
\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 > plot(gold_target1[['0_1']]~gold_target1[[\'91X\'92]])\
\

\b c)\
\

\b0 Fit with logistic model.
\b \
\

\b0 > glm.out = glm(gold_target1[['0_1']] ~ gold_target1[['X']], 	family=binomial(logit), data=gold_target1)\
\

\b d) 
\b0 \
\
Visualize model.\
\
> lines(gold_target1[['X']], glm.out$fitted, type="l", col="red")\
\

\b e)\
\

\b0 Fitting column 4 versus 1 and 2\
\
> glm.out = glm(gold_target1[['0_1']] ~ gold_target1[['X']]+gold_target1[['Y']], 	family=binomial(logit), data=gold_target1)\
\

\b f)\
\

\b0 The estimated coefficients for these two models are different in that the coefficient for (column4 ~ column2) is less than the coefficient for (column4 ~ column1 + column2). \
\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 (column4 ~ column2): 1.7427\
(column4 ~ column1 + column2): 0.09190\
\
- The odds of a presence or absence of lineament are 5.712747 more likely according to (column4 ~ column2) model.\
- The odds of a presence or absence of lineament are 1.096255 more likely according to (column4 ~ column1 + column2) model.\

\f1 \cf2 \expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 \

\itap1\trowd \taflags0 \trgaph108\trleft-108 \trcbpat4 \trbrdrt\brdrnil \trbrdrl\brdrnil \trbrdrr\brdrnil \trpadl120 \tapadb160 
\clvertalt \clshdrawnil \clwWidth19820\clftsWidth3 \clbrdrt\brdrnil \clbrdrl\brdrnil \clbrdrb\brdrnil \clbrdrr\brdrnil \clpadl0 \clpadr0 \gaph\cellx8640
\pard\intbl\itap1\pardeftab720\sl340\partightenfactor0
\cf3 \strokec3 \
\pard\intbl\itap1\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf3 \
\pard\intbl\itap1\pardeftab720\sl340\partightenfactor0
\cf3 \
\cell \row

\itap1\trowd \taflags0 \trgaph108\trleft-108 \trcbpat4 \trbrdrl\brdrnil \trbrdrt\brdrnil \trbrdrr\brdrnil \trpadl120 \tapadb160 
\clvertalt \clshdrawnil \clwWidth19820\clftsWidth3 \clbrdrt\brdrnil \clbrdrl\brdrnil \clbrdrb\brdrnil \clbrdrr\brdrnil \clpadl0 \clpadr0 \gaph\cellx8640

\itap2\trowd \taflags0 \trgaph108\trleft-108 \trbrdrt\brdrnil \trbrdrl\brdrnil \trbrdrt\brdrnil \trbrdrr\brdrnil 
\clvertalt \clshdrawnil \clwWidth19820\clftsWidth3 \clbrdrt\brdrnil \clbrdrl\brdrnil \clbrdrb\brdrnil \clbrdrr\brdrnil \clpadl0 \clpadr0 \gaph\cellx8640
\pard\intbl\itap2\pardeftab720\sl340\partightenfactor0
\cf3 \nestcell \lastrow\nestrow\cell \lastrow\row
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f0 \cf0 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 \
\
\
\
\
\
\
\
\
\
}